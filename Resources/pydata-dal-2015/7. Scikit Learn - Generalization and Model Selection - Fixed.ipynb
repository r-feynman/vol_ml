{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "- How to generalize better?\n",
    "- What can we do to make the machine learning model generalizes on the test data as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection is the task of choosing best performing classifier out of a candidate set of classifiers which is very useful for libraries like Scikit-Learn as the api for different parameters stay across different classifiers. That makes it very easy to try different \"estimators\" in the same dataset. \n",
    "\n",
    "After learning about parameter search, one could do parameter search on a given classifier, but in practice you rarely depend on one classifier. Ideally, you need to compare different classifiers (as their learning function differs a lot), their performance on your real dataset may vary drastically. In order to choose not best paremeters for a given classifier, one needs to look at different classifiers and then do a parameters search on top of the those multiple classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection makes sure that not only I will get the best parameters but also best classifier for a given candidate of classifiers for a scoring function that I will optimize for. But this is not good enough because I want to also see the effectof total number of features as well; when I change the total number of features, how does it affect the score of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mkangrga\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import feature_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "_DATA_DIR = 'data'\n",
    "_NYT_DATA_PATH = os.path.join(_DATA_DIR, 'nyt_title_data.csv')\n",
    "\n",
    "_PLT_LEGEND_OPTIONS = dict(loc=\"upper center\", \n",
    "                           bbox_to_anchor=(0.5, -0.15),\n",
    "                           fancybox=True, \n",
    "                           shadow=True, \n",
    "                           ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = [ii.strip() for ii in '#30a2da, #fc4f30, #e5ae38, #6d904f, #8b8b8b'.split(',')]\n",
    "colors += ['#' + ii.strip() for ii in '348ABD, A60628, 7A68A6, 467821,D55E00,  CC79A7, 56B4E9, 009E73, F0E442, 0072B2'.split(',')]\n",
    "markers = itertools.cycle([\"o\", \"D\"])\n",
    "colors = itertools.cycle(colors)\n",
    "\n",
    "def cv(X, y, clf, nfeats, clfname, scoring=metrics.accuracy_score, n_folds=10):\n",
    "  stratified_k_fold = cross_validation.StratifiedKFold(y, n_folds=n_folds)\n",
    "  accuracy, ii = 0., 0\n",
    "  for train, test in stratified_k_fold:\n",
    "    ii += 1\n",
    "    X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = scoring(y_test, y_pred)\n",
    "    accuracy += score\n",
    "  accuracy /= float(n_folds)\n",
    "  return accuracy\n",
    "\n",
    "def plot_accuracies(accuracies, xvals, legends):\n",
    "  fig = plt.figure(figsize=(16, 12))\n",
    "  ax = fig.add_subplot(111)\n",
    "  for ii in range(0, accuracies.shape[0]):\n",
    "    ax.plot(xvals, accuracies[ii, :], color=next(colors), marker=next(markers), label=legends[ii])\n",
    "  plt.xlabel(\"Number of Features\")\n",
    "  plt.ylabel(\"Accuracy\")\n",
    "  plt.title(\"Accuracy vs Number of Features\")\n",
    "  ax.set_xscale(\"log\")\n",
    "  box = ax.get_position()\n",
    "  ax.set_position([box.x0, box.y0 + box.height * 0.3, box.width, box.height * 0.7])\n",
    "  ax.legend(**_PLT_LEGEND_OPTIONS)\n",
    "  plt.show()\n",
    "\n",
    "def estimator_name(clf):\n",
    "  return type(clf).__name__\n",
    "    \n",
    "def select_model(X, y, scoring=metrics.accuracy_score):\n",
    "  n_features = np.array([10, 100, 500, 1000, 5000, 10000, 20000, 50000, 100000])\n",
    "  clfs = [\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.MultinomialNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "    tree.DecisionTreeClassifier(),\n",
    "    ensemble.RandomForestClassifier(n_estimators=10),\n",
    "    svm.LinearSVC(random_state=0),\n",
    "    linear_model.LogisticRegression(),\n",
    "    linear_model.SGDClassifier(),\n",
    "    linear_model.RidgeClassifier(),\n",
    "  ]\n",
    "  \n",
    "  classifier_names = map(estimator_name, clfs)\n",
    "  feature_selection_methods = [feature_selection.f_classif]\n",
    "  accuracies = np.zeros((len(clfs), len(n_features), len(feature_selection_methods)))\n",
    "  for kk in range(len(feature_selection_methods)):\n",
    "    X_feature_selected = X.copy().toarray()\n",
    "    for jj in range(len(n_features)):\n",
    "      for ii in range(len(clfs)):\n",
    "        accuracies[ii, jj, kk] = cv(X_feature_selected, y, clfs[ii], n_features[jj], classifier_names[ii], scoring=scoring)\n",
    "  for k in range(len(feature_selection_methods)):\n",
    "    for i in range(len(clfs)):\n",
    "      print (\"%22s \" % classifier_names[i])\n",
    "      for j in range(accuracies.shape[1]):\n",
    "        print(\"%5.3f\" % accuracies[i, j, k])\n",
    "      print(plot_accuracies(accuracies[:, :, k], n_features, classifier_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(_NYT_DATA_PATH) as nyt:\n",
    "  nyt_data = []\n",
    "  nyt_labels = []\n",
    "  csv_reader = csv.reader(nyt)\n",
    "  for line in csv_reader:\n",
    "    nyt_labels.append(int(line[0]))\n",
    "    nyt_data.append(line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram & Bigram for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example string:      Clinton Defends His Policies In Kosovo, China and Mexico\n",
      "Preprocessed string: clinton defends his policies in kosovo, china and mexico\n",
      "Tokenized string:    ['Clinton', 'Defends', 'His', 'Policies', 'In', 'Kosovo', 'China', 'and', 'Mexico']\n",
      "N-gram data string:  ['clinton', 'defends', 'policies', 'kosovo', 'china', 'mexico', 'clinton defends', 'defends policies', 'policies kosovo', 'kosovo china', 'china mexico']\n"
     ]
    }
   ],
   "source": [
    "X = np.array([''.join(el) for el in nyt_data])\n",
    "y = np.array([el for el in nyt_labels])\n",
    "\n",
    "\n",
    "vectorizer = text.TfidfVectorizer(min_df=2, \n",
    "                                  ngram_range=(1, 2), \n",
    "                                  stop_words='english', \n",
    "                                  strip_accents='unicode', \n",
    "                                  norm='l2')\n",
    " \n",
    "example = X[5]\n",
    "\n",
    "print(\"Example string:      {}\".format(example))\n",
    "print(\"Preprocessed string: {}\".format(vectorizer.build_preprocessor()(example)))\n",
    "print(\"Tokenized string:    {}\".format(str(vectorizer.build_tokenizer()(example))))\n",
    "print(\"N-gram data string:  {}\".format(str(vectorizer.build_analyzer()(example))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TfidfVectorizer, one may choose the `ngram_range`, in this example, it is bigram and unigram. Choosing bigram enables us to capture phrases that could play an important role in determining the class of the observation in the dataset. If you believe, even a longer sequence of words `internet of things` play an important role, you could pass `ngram_range` to `(1, 3)` to include trigrams as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dole Courts Democrats', 'Yanks End Drought; Mets Fall in Opener',\n",
       "       'Lumpectomies Seen As Equal in Benefit To Breast Removals', ...,\n",
       "       'Delays Hurting U.S. Rebuilding In Afghanistan',\n",
       "       'SENATE APPROVES $1 BILLION TO AID COLOMBIA MILITARY',\n",
       "       'POLITICS: THE MONEY; A Hollywood Production: Political Money'], \n",
       "      dtype='<U127')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2161x2171 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10377 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'map' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fa45aceb0f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselect_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-308c4817877c>\u001b[0m in \u001b[0;36mselect_model\u001b[0;34m(X, y, scoring)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0maccuracies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_feature_selected\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_selection_methods\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'map' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "select_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot not only shows the relative strength of classifiers with respect to each for a given number of features but also provides a good picture on the effect of number of features for the classification task at the hand. Generally even if Scikit-Learn has all grid search other capabilities to output the best estimator for a given classifier or pipeline, I'd prefer some plot over those grids. You not only learn the best parameters but also effects of classifier and feature number in a one plot. \n",
    "And this could be easily adopted to different number of paramters(instead of number of features) for classifiers that share a number of parameters in the optimization to see the effect of each parameter visually rather than getting the best parameters. \n",
    "This way you could both explore and gain insights about parameters that the classifier has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_model(X, y, scoring=metrics.f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
