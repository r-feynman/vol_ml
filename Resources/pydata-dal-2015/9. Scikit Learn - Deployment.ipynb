{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence of Vectorizer and Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, we are doing first training and then using the vectorizer, feature selector and classifier immediately after training. However, in practice this scenario is one of the least likely occurring one. You generally train your classifier once, and then you want to use that as much as you'd like. In order to do so, we need to persist the vectorizer, feature selector and classifier. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last notebook, I showed that `Pipeline` structure gives a nice way to combine and put together a single component for all vectorizer, feature selector and also classifier(for the sequential pipeline). Instead of serializing two and possibly three structures, we would use `pipeline` to handle the serialization and deserialization. Without loss of generality, the things that I will show in this notebook is applicable to independent and separate components of the system(namely vectorizer, fature selector and classifier) as well. However, `pipeline` is preffered way to persist your whole __machine learning pipeline__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import feature_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import pipeline\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from sklearn import externals\n",
    "\n",
    "_DATA_DIR = 'data'\n",
    "_NYT_DATA_PATH = os.path.join(_DATA_DIR, 'nyt_title_data.csv')\n",
    "_SERIALIZATION_DIR = 'serializations'\n",
    "_SERIALIZED_PIPELINE_NAME = 'pipe.pickle'\n",
    "_SERIALIZATION_PATH = os.path.join(_SERIALIZATION_DIR, _SERIALIZED_PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(_NYT_DATA_PATH) as nyt:\n",
    "    nyt_data = []\n",
    "    nyt_labels = []\n",
    "    csv_reader = csv.reader(nyt)\n",
    "    for line in csv_reader:\n",
    "      nyt_labels.append(int(line[0]))\n",
    "      nyt_data.append(line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([''.join(el) for el in nyt_data])\n",
    "y = np.array([el for el in nyt_labels])\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y)\n",
    "\n",
    "vectorizer = text.TfidfVectorizer(min_df=2, \n",
    " ngram_range=(1, 2), \n",
    " stop_words='english', \n",
    " strip_accents='unicode', \n",
    " norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = pipeline.Pipeline([(\"vectorizer\", vectorizer), (\"svm\", linear_model.RidgeClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf...copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to here, everything is same with previous notebook, there should be no surprises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Serialization Directory if it does not exist already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(_SERIALIZATION_DIR):\n",
    "    os.makedirs(_SERIALIZATION_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['serializations/pipe.pickle',\n",
       " 'serializations/pipe.pickle_01.npy',\n",
       " 'serializations/pipe.pickle_02.npy',\n",
       " 'serializations/pipe.pickle_03.npy',\n",
       " 'serializations/pipe.pickle_04.npy',\n",
       " 'serializations/pipe.pickle_05.npy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "externals.joblib.dump(pipe, _SERIALIZATION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  joblib.dump returns a list of filenames. Each individual numpy array contained in the clf object is serialized as a separate file on the filesystem. All files are required in the same folder when reloading the model with joblib.load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this point, the serialization is complete and ready for deployment. If we were not using `Pipeline`, we would need at least two serialization for vectorizer and classifier(feature seelector would be third if one uses that). In order to deploy the model, let's deserialize the `pipeline` in  a very similar manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = externals.joblib.load(_SERIALIZATION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf...copy_X=True, fit_intercept=True,\n",
       "        max_iter=None, normalize=False, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully persisted our pipeline and loaded into the namespace again, ready to apply our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Never, ever unpickle untrusted data! `Pickle` (the serialization method) in Python which `joblib` uses under the hood has a lot of issues in terms security and vulnerability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the pipeline on the test dataset if it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 19, 16, 19, 16, 16, 19, 19, 19, 19, 20, 16, 19, 16, 16, 16, 19,\n",
       "       19, 16, 19, 19, 19, 19,  3, 19, 15, 19, 12, 19, 15, 15, 12, 15,  3,\n",
       "       20, 16,  3, 19, 20, 16, 19, 16, 19, 19, 20, 16, 29, 19, 20, 16, 19,\n",
       "       16, 20, 19, 19, 20, 19,  3, 29, 16, 16, 19, 20, 20, 15,  3, 12, 16,\n",
       "       19, 19, 16, 19, 19, 16, 16, 16, 29, 19, 20, 19, 19, 20, 19, 19, 20,\n",
       "       16, 20, 19, 19, 19, 29, 16, 20, 19, 19, 16, 29, 19, 12, 19, 16, 16,\n",
       "       19, 19, 16, 19, 29, 20, 16, 19, 19, 19, 15, 20, 19, 20, 20, 16, 19,\n",
       "        3, 16, 19, 20, 19, 16, 19, 19, 29, 19, 19, 19, 20, 15, 19, 16, 20,\n",
       "       16, 20, 19, 20, 19, 20, 29, 16, 19, 16, 16, 19, 12, 20, 19, 19, 19,\n",
       "       20, 16, 19, 12, 20, 19, 15, 20, 16, 16, 19, 19, 15, 29, 20, 12, 16,\n",
       "       20, 15, 19, 12, 20, 19, 19, 20, 12, 20, 16, 20, 20, 16, 20, 19, 19,\n",
       "       19, 19, 16, 19, 19,  3, 16, 20, 19, 16, 20, 12, 16, 16,  3, 16, 19,\n",
       "       20, 20, 19, 19, 19, 20, 19, 16, 16, 12, 19, 19, 12,  3, 16, 29, 16,\n",
       "       12, 16, 19,  3,  3, 19, 19, 19, 15, 15, 29, 16, 20, 20, 12, 19, 19,\n",
       "       19, 12, 19, 16, 19, 29, 19, 20, 20, 19, 16, 19,  3, 20, 19, 19,  3,\n",
       "       16, 19, 16, 16, 16, 16, 16, 16, 16, 15, 16, 19, 16, 16, 20, 19, 29,\n",
       "       16, 19,  3, 16, 16, 19, 16, 16, 16, 15, 19, 16, 16, 19, 19, 20, 19,\n",
       "       20, 20, 12, 19,  3, 16, 19, 19, 16, 19, 20, 19, 20,  3, 19, 16, 19,\n",
       "        3, 19, 19, 16, 20, 16, 20, 29, 19, 19, 29, 15, 16, 19, 16, 29, 19,\n",
       "       29, 16, 20, 19, 16, 20, 19, 19, 19, 19, 16, 15, 19, 16, 12, 19, 19,\n",
       "       16, 19, 19,  3, 29, 19, 12, 19, 20, 29, 15,  3, 16, 16, 19, 16,  3,\n",
       "       19, 15,  3, 20, 19, 19, 20, 19, 19, 19,  3, 20, 20, 29, 19, 15, 19,\n",
       "       16, 19, 19, 19, 20, 16, 19, 16, 20,  3, 20, 19, 20, 15, 19, 19,  3,\n",
       "       20, 20, 19, 16, 15, 20, 20, 20, 16, 19, 16, 16,  3, 19, 19, 20, 20,\n",
       "       19, 19, 19, 12, 12, 19, 15, 16, 16, 16, 20, 15, 16, 19, 19, 20, 20,\n",
       "       20, 12, 20, 19, 19, 19, 19, 20, 15, 16, 19, 20, 15, 20, 19, 15, 19,\n",
       "       19, 12, 20, 19, 16, 12, 19, 19, 15, 20, 19, 19, 16,  3, 19, 19, 16,\n",
       "       29, 19, 16, 16, 20, 16,  3, 19, 29, 15, 12, 12, 29, 19, 15, 19, 20,\n",
       "       29, 20, 19, 19, 20, 19, 16, 19, 20,  3, 16, 19, 19, 20, 16, 29, 19,\n",
       "       20, 16, 20,  3, 20, 16, 16, 16, 19,  3, 20, 20, 20, 16, 19, 16, 19,\n",
       "       12,  3, 16, 19, 19, 16, 19, 19, 29, 16, 16, 16, 20, 19, 16, 16, 16,\n",
       "       15, 15, 20, 29, 19, 19, 16, 15, 16, 16, 16, 16, 20, 19])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "- `Pipeline` is not just useful for abstraction but also easier to maintain, persist and deploy.\n",
    "- Do not use a loss-compression technique to compress serialization.(Tarballs would work fine)\n",
    "- If you want to create exact environment of your training set, use always `virtualenv` and note the version numbers of the libraries that you are using.(See the Notebook 0)\n",
    "- Some of the algorithms support `partial_fit` function for online learning. ([SGD Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html), [Perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron), [MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)). If you have incremental data that you want to improve your classifier over time, you may want to persist your models and then use `partial_fit` to improve them when you have new data. Works like a charm!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
